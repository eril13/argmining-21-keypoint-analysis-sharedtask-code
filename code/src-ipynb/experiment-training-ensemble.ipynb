{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import GroupKFold\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload\n",
    "\n",
    "import sys\n",
    "import os\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '2'\n",
    "sys.path.insert(0, \"../src-py\")\n",
    "\n",
    "import sbert_training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_df = pd.read_csv('../../data/training_df.csv')\n",
    "valid_df = pd.read_csv('../../data/our_valid.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_kp_df     = pd.read_csv('../../KPA_2021_shared_task/kpm_data/key_points_train.csv')\n",
    "train_arg_df    = pd.read_csv('../../KPA_2021_shared_task/kpm_data/arguments_train.csv')\n",
    "train_labels_df = pd.read_csv('../../KPA_2021_shared_task/kpm_data/labels_train.csv')\n",
    "\n",
    "dev_kp_df     = pd.read_csv('../../KPA_2021_shared_task/kpm_data/key_points_dev.csv')\n",
    "dev_arg_df    = pd.read_csv('../../KPA_2021_shared_task/kpm_data/arguments_dev.csv')\n",
    "dev_labels_df = pd.read_csv('../../KPA_2021_shared_task/kpm_data/labels_dev.csv')\n",
    "\n",
    "full_train_kp_df = pd.concat([train_kp_df,dev_kp_df])\n",
    "full_train_arg_df  = pd.concat([train_arg_df,dev_arg_df])\n",
    "full_train_labels_df = pd.concat([train_labels_df,dev_labels_df])\n",
    "\n",
    "test_kp_df     = pd.read_csv('../../KPA_2021_shared_task/test_data/key_points_test.csv')\n",
    "test_arg_df    = pd.read_csv('../../KPA_2021_shared_task//test_data/arguments_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_train_df = full_train_labels_df.merge(full_train_arg_df, how='inner', left_on='arg_id', right_on='arg_id')\n",
    "all_train_df = all_train_df.merge(full_train_kp_df[['key_point_id', 'key_point']], how='inner', left_on='key_point_id', right_on='key_point_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_arg_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_kp_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_argument_with_keypoints(result, kp_dict, arg_dict):\n",
    "    \n",
    "    for arg, arg_embedding in arg_dict.items():\n",
    "        result[arg] = {}\n",
    "        for kp, kp_embedding in kp_dict.items():\n",
    "            result[arg][kp] = util.pytorch_cos_sim(arg_embedding, kp_embedding).item()\n",
    "        \n",
    "        #Applying softmax\n",
    "        kp_scores = list(result[arg].items())\n",
    "        kp_ids, kp_scores = zip(*kp_scores)\n",
    "        #print(kp_ids)\n",
    "        #print(kp_scores)\n",
    "        #kp_scores = torch.softmax(torch.Tensor(kp_scores), 0).tolist()\n",
    "        #print(kp_scores)\n",
    "        result[arg] = {kp_id:score for kp_id, score in zip(kp_ids, kp_scores)}\n",
    "        \n",
    "\n",
    "    return result\n",
    "\n",
    "def predict(model, argument_df, keypoint_df, output_path, append_topic=False):\n",
    "    argument_keypoints = {}\n",
    "    for topic in argument_df.topic.unique():\n",
    "        for stance in [-1, 1]:\n",
    "            topic_keypoints_ids = keypoint_df[(keypoint_df.topic==topic) & (keypoint_df.stance==stance)]['key_point_id'].tolist()\n",
    "            topic_keypoints = keypoint_df[(keypoint_df.topic==topic) & (keypoint_df.stance==stance)]['key_point'].tolist()\n",
    "            if append_topic:\n",
    "                topic_keypoints = [topic + ' <SEP> ' + x for x in topic_keypoints]\n",
    "                \n",
    "            topic_keypoints_embeddings = model.encode(topic_keypoints)\n",
    "            topic_kp_embed = dict(zip(topic_keypoints_ids, topic_keypoints_embeddings))\n",
    "\n",
    "            topic_arguments_ids = argument_df[(argument_df.topic==topic) & (argument_df.stance==stance)]['arg_id'].tolist()\n",
    "            topic_arguments = argument_df[(argument_df.topic==topic) & (argument_df.stance==stance)]['argument'].tolist()\n",
    "            topic_arguments_embeddings = model.encode(topic_arguments)\n",
    "            topic_arg_embed= dict(zip(topic_arguments_ids, topic_arguments_embeddings))\n",
    "\n",
    "            argument_keypoints = match_argument_with_keypoints(argument_keypoints, topic_kp_embed, topic_arg_embed)\n",
    "    \n",
    "    json.dump(argument_keypoints, open(output_path, 'w'))\n",
    "    \n",
    "    return argument_keypoints\n",
    "\n",
    "def predict_and_evaluate(argument_df, keypoint_df, gold_data_dir, subset_name):\n",
    "    pred_df = {}\n",
    "    for model_path in models_list:\n",
    "        append_topic= 'topic_added' in model_path\n",
    "        #Predict\n",
    "        model = SentenceTransformer(model_path)\n",
    "        model_name = model_path.split('/')[-1]\n",
    "        predictions_file = pred_output_path+model_name+ '-' + subset_name + '-preds.json'\n",
    "        json_preds = predict(model, argument_df, keypoint_df, predictions_file, append_topic)\n",
    "\n",
    "        #Evaluate\n",
    "        arg_df, kp_df, labels_df = load_kpm_data(gold_data_dir, subset=subset_name)\n",
    "        merged_df = get_predictions(predictions_file, labels_df, arg_df)\n",
    "        print('Evaluating {}:'.format(model_name))\n",
    "        evaluate_predictions(merged_df)\n",
    "        \n",
    "        pred_df[model_name] = merged_df\n",
    "\n",
    "    return pred_df\n",
    "\n",
    "def predict_models(argument_df, keypoint_df, gold_data_dir, subset_name):\n",
    "    pred_df = {}\n",
    "    for model_path in models_list:\n",
    "        append_topic= 'topic_added' in model_path\n",
    "        #Predict\n",
    "        model = SentenceTransformer(model_path)\n",
    "        model_name = model_path.split('/')[-1]\n",
    "        predictions_file = pred_output_path+model_name+ '-' + subset_name + '-preds.json'\n",
    "        json_preds = predict(model, argument_df, keypoint_df, predictions_file, append_topic)\n",
    "\n",
    "        #Evaluate\n",
    "        arg_df, kp_df, labels_df = load_kpm_data(gold_data_dir, subset=subset_name)\n",
    "        merged_df = get_predictions(predictions_file, labels_df, arg_df)\n",
    "        #print('Evaluating {}:'.format(model_name))\n",
    "        #evaluate_predictions(merged_df)\n",
    "        \n",
    "        pred_df[model_name] = merged_df\n",
    "\n",
    "    return pred_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skf = GroupKFold(n_splits=5)\n",
    "fold = -1\n",
    "for train_index, test_index in skf.split(all_train_df, groups=all_train_df.topic):\n",
    "    fold += 1\n",
    "    tmp_train_df, tmp_test_df = all_train_df.iloc[train_index], all_train_df.iloc[test_index]\n",
    "\n",
    "    df = tmp_train_df.copy()\n",
    "    df['keypoint'] = df.apply(lambda x: x['topic'] + ' <SEP> ' + x['key_point'], axis=1)\n",
    "    df['label'] = df.label.apply(lambda x: int(x))\n",
    "    df[['argument', 'keypoint', 'label']].to_csv('/workspace/ceph_data/data-in-progress/data-research/arguana/arg-generation/keypoint-analysis-sharedtask/siamese-data/training_df_contrastive-fold-{}.csv'.format(fold))\n",
    "    \n",
    "    df = tmp_test_df.copy()\n",
    "    df['keypoint'] = df.apply(lambda x: x['topic'] + ' <SEP> ' + x['key_point'], axis=1)\n",
    "    df['label'] = df.label.apply(lambda x: int(x))\n",
    "    df[['argument', 'keypoint', 'label']].to_csv('/workspace/ceph_data/data-in-progress/data-research/arguana/arg-generation/keypoint-analysis-sharedtask/siamese-data/valid_df_contrastive-fold-{}.csv'.format(fold))\n",
    "\n",
    "    tmp_test_key_points_df = tmp_test_df[['key_point_id', 'key_point', 'topic', 'stance']].drop_duplicates()\n",
    "    tmp_test_arguments_df = tmp_test_df[['arg_id', 'argument', 'topic', 'stance']].drop_duplicates()\n",
    "    tmp_test_labels_df = tmp_test_df[['arg_id', 'key_point_id', 'label']]\n",
    "    tmp_test_key_points_df.to_csv('../../data/cross-validation/key_points_test.csv')\n",
    "    tmp_test_arguments_df.to_csv('../../data/cross-validation/arguments_test.csv')\n",
    "    tmp_test_labels_df.to_csv('../../data/cross-validation/labels_test.csv')\n",
    "\n",
    "\n",
    "    sbert_training.train_model('/workspace/ceph_data/data-in-progress/data-research/arguana/arg-generation/keypoint-analysis-sharedtask/siamese-data/',\n",
    "                            '../../data/cross-validation/',\n",
    "                            'test',\n",
    "                            '/workspace/ceph_data/data-in-progress/data-research/arguana/arg-generation/keypoint-analysis-sharedtask/siamese-models/',\n",
    "                            'roberta-large',\n",
    "                            model_suffix='contrastive-10-epochs-fold-{}'.format(fold), \n",
    "                            data_file_suffix='contrastive-fold-{}'.format(fold), \n",
    "                            num_epochs=1, max_seq_length=70, add_special_token=True, train_batch_size=32, loss='ContrastiveLoss')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_list = [\n",
    "    '/workspace/ceph_data/data-in-progress/data-research/arguana/arg-generation/keypoint-analysis-sharedtask/siamese-models/roberta-base-contrastive-10-epochs-fold-fold-0-2021-06-24_14-14-22',\n",
    "     '/workspace/ceph_data/data-in-progress/data-research/arguana/arg-generation/keypoint-analysis-sharedtask/siamese-models/roberta-base-contrastive-10-epochs-fold-fold-1-2021-06-24_14-18-39',\n",
    "     '/workspace/ceph_data/data-in-progress/data-research/arguana/arg-generation/keypoint-analysis-sharedtask/siamese-models/roberta-base-contrastive-10-epochs-fold-fold-2-2021-06-24_14-22-54',\n",
    "     '/workspace/ceph_data/data-in-progress/data-research/arguana/arg-generation/keypoint-analysis-sharedtask/siamese-models/roberta-base-contrastive-10-epochs-fold-fold-3-2021-06-24_14-26-51',\n",
    "     '/workspace/ceph_data/data-in-progress/data-research/arguana/arg-generation/keypoint-analysis-sharedtask/siamese-models/roberta-base-contrastive-10-epochs-fold-4-2021-06-24_14-56-42',\n",
    "]\n",
    "\n",
    "pred_output_path = '/workspace/ceph_data/data-in-progress/data-research/arguana/arg-generation/keypoint-analysis-sharedtask/siamese-data/preds/'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, InputExample, LoggingHandler, losses, models, util\n",
    "import torch\n",
    "from track_1_kp_matching import *\n",
    "# testing wheter prediction work on the dev data\n",
    "test_keypoints_df = pd.read_csv('/workspace/project_git/keypoint-analysis-sharedtask/KPA_2021_shared_task/kpm_data/key_points_dev.csv')\n",
    "test_arguments_df = pd.read_csv('/workspace/project_git/keypoint-analysis-sharedtask/KPA_2021_shared_task/kpm_data/arguments_dev.csv')\n",
    "preds_df = predict_and_evaluate(test_arguments_df, test_keypoints_df,  '/workspace/project_git/keypoint-analysis-sharedtask/KPA_2021_shared_task/kpm_data', 'dev')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, InputExample, LoggingHandler, losses, models, util\n",
    "import torch\n",
    "from track_1_kp_matching import *\n",
    "# predicting the test data\n",
    "test_keypoints_df = pd.read_csv('/workspace/project_git/keypoint-analysis-sharedtask/KPA_2021_shared_task/test_data/key_points_test.csv')\n",
    "test_arguments_df = pd.read_csv('/workspace/project_git/keypoint-analysis-sharedtask/KPA_2021_shared_task/test_data/arguments_test.csv')\n",
    "preds_df = predict_models(test_arguments_df, test_keypoints_df,  '../../data/cross-validation', 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(preds_df['roberta-base-contrastive-10-epochs-fold-fold-0-2021-06-24_14-14-22'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = []\n",
    "for model in models_list:\n",
    "    scores.append(preds_df[model.split('/')[-1]].score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(np.mean(scores,axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
